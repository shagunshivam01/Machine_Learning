{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f570b9c1-02b8-4dc2-81cf-32f4e06e74aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ea11cbdb354871a4913ddaade06214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/9171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imagefolder\", data_dir = './foodimg')\n",
    "ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87743c61-703b-4210-b6dc-e80ca33bb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece8430-7cfc-4a5b-95c8-88741bdcdc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 7335\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1834\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175895f5-d65d-4ff0-85ad-bdb89aace41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3479/3366162530.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821ad6be-e0f6-482f-adf1-c7b9f89cb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['train'].features['label'].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcd2fcc-be6c-4cd8-8197-3d0688412f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:35:53.285643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 15:35:53.577615: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 15:35:54.340971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shagun/miniconda3/envs/tf/lib/\n",
      "2024-07-24 15:35:54.341074: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shagun/miniconda3/envs/tf/lib/\n",
      "2024-07-24 15:35:54.341082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "model_name_or_path = 'microsoft/resnet-18'\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b600b4f8-c9d7-4ae0-ac64-8bcd8a2ed572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "size = feature_extractor.size  # why this code block\n",
    "if \"height\" in size:\n",
    "    crop_size = (size[\"height\"], size[\"width\"])\n",
    "    resize_size = (size[\"height\"], size[\"width\"])\n",
    "elif \"shortest_edge\" in size:\n",
    "    crop_size = resize_size = size[\"shortest_edge\"]\n",
    "\n",
    "normalize = Normalize(mean = feature_extractor.image_mean, std = feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(resize_size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946c2087-1637-43bb-84b7-0381efe340f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data['train']\n",
    "val_ds = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4371b898-5343-45c7-a9ab-88a1d58a963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a73606b-8f01-44a4-a82e-81999ea50fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=751x532>,\n",
       " 'label': 16,\n",
       " 'pixel_values': tensor([[[-0.9705, -1.1760, -1.5357,  ..., -1.6898, -1.8782, -1.8782],\n",
       "          [-0.7993, -1.1589, -1.2617,  ..., -1.6727, -1.5528, -1.5528],\n",
       "          [-0.5082, -0.6281, -0.6794,  ..., -1.4843, -1.4329, -1.2788],\n",
       "          ...,\n",
       "          [-1.6727, -1.7240, -1.6727,  ...,  0.9646,  0.9132,  0.8789],\n",
       "          [-1.5870, -1.6555, -1.6384,  ...,  0.9646,  0.9474,  0.9132],\n",
       "          [-1.5699, -1.5699, -1.5870,  ...,  0.9474,  0.9474,  0.9474]],\n",
       " \n",
       "         [[-1.1078, -1.2479, -1.5280,  ..., -1.4930, -1.6856, -1.7556],\n",
       "          [-0.9678, -1.2654, -1.3179,  ..., -1.5455, -1.4405, -1.4755],\n",
       "          [-0.7227, -0.7752, -0.7577,  ..., -1.5280, -1.4755, -1.3179],\n",
       "          ...,\n",
       "          [-1.5630, -1.6155, -1.5630,  ...,  0.8179,  0.7654,  0.7304],\n",
       "          [-1.4755, -1.5630, -1.5455,  ...,  0.8179,  0.8004,  0.7654],\n",
       "          [-1.4930, -1.4930, -1.5105,  ...,  0.8004,  0.8004,  0.8004]],\n",
       " \n",
       "         [[-1.1596, -1.2467, -1.4907,  ..., -1.4036, -1.5081, -1.5256],\n",
       "          [-1.0376, -1.2816, -1.2293,  ..., -1.4907, -1.3164, -1.3164],\n",
       "          [-0.8110, -0.8284, -0.7413,  ..., -1.4559, -1.3861, -1.1944],\n",
       "          ...,\n",
       "          [-1.6302, -1.6824, -1.6127,  ...,  0.7576,  0.6879,  0.6356],\n",
       "          [-1.5604, -1.6302, -1.6127,  ...,  0.7576,  0.7228,  0.6531],\n",
       "          [-1.5953, -1.6127, -1.6127,  ...,  0.7402,  0.7228,  0.7054]]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97765afb-8dcb-4544-819b-d4d0b42dcc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([28]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([28, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ResNetForImageClassification\n",
    "# model_name_or_path = 'microsoft/resnet-18'\n",
    "\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd23ef1-e740-4457-8e63-23e7009d767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir = './results',\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  num_train_epochs=10,\n",
    "  fp16=True,\n",
    "  # save_steps=100,\n",
    "  # eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  # push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    "  # hub_strategy=\"end\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ab57d4-7fd7-415f-99b8-aaae4ecd97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5d5010-897e-4745-9142-e60c2cd1291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de96986-c6c0-42a7-aeb2-bd2c9beaca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7d0b757-f3d8-46e3-9b65-6820d476ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shagun/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4590' max='4590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4590/4590 16:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.298400</td>\n",
       "      <td>0.953292</td>\n",
       "      <td>0.721374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>0.713048</td>\n",
       "      <td>0.787350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>0.638691</td>\n",
       "      <td>0.816794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.575935</td>\n",
       "      <td>0.846238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.856052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.461464</td>\n",
       "      <td>0.868593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>0.442861</td>\n",
       "      <td>0.871320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.455999</td>\n",
       "      <td>0.863686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.400323</td>\n",
       "      <td>0.888768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.394745</td>\n",
       "      <td>0.883860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        10.0\n",
      "  total_flos               = 690451016GF\n",
      "  train_loss               =      0.7512\n",
      "  train_runtime            =  0:16:09.26\n",
      "  train_samples_per_second =      75.676\n",
      "  train_steps_per_second   =       4.736\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model('./food_classification')\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a78c3141-6f00-4dca-879c-d9ad59cd9853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =     0.8839\n",
      "  eval_loss               =     0.3947\n",
      "  eval_runtime            = 0:00:18.73\n",
      "  eval_samples_per_second =     97.867\n",
      "  eval_steps_per_second   =     12.273\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31587db4-cfd8-40f5-9573-0e1c52b28655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ae622d-d7e1-44ce-9a82-733996345b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs = {\n",
    "#     \"finetuned_from\": model.config._name_or_path,\n",
    "#     \"tasks\": \"image-classification\",\n",
    "#     \"dataset\": 'food_images',\n",
    "#     \"tags\": ['image-classification'],\n",
    "# }\n",
    "\n",
    "# trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc16d83-3403-4a64-8d33-59c99a31197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetForImageClassification(\n",
       "  (resnet): ResNetModel(\n",
       "    (embedder): ResNetEmbeddings(\n",
       "      (embedder): ResNetConvLayer(\n",
       "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (encoder): ResNetEncoder(\n",
       "      (stages): ModuleList(\n",
       "        (0): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBasicLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBasicLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBasicLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBasicLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBasicLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBasicLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBasicLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBasicLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df91b6-e51a-4a63-81b5-cc9eb4dee82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
