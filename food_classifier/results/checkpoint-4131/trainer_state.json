{
  "best_metric": 0.4003233015537262,
  "best_model_checkpoint": "./results/checkpoint-4131",
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 4131,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02178649237472767,
      "grad_norm": 7.7541728019714355,
      "learning_rate": 0.00019956427015250546,
      "loss": 3.297,
      "step": 10
    },
    {
      "epoch": 0.04357298474945534,
      "grad_norm": 7.1290130615234375,
      "learning_rate": 0.0001991285403050109,
      "loss": 3.277,
      "step": 20
    },
    {
      "epoch": 0.06535947712418301,
      "grad_norm": 7.624969482421875,
      "learning_rate": 0.00019869281045751635,
      "loss": 3.072,
      "step": 30
    },
    {
      "epoch": 0.08714596949891068,
      "grad_norm": 9.029882431030273,
      "learning_rate": 0.0001982570806100218,
      "loss": 3.0684,
      "step": 40
    },
    {
      "epoch": 0.10893246187363835,
      "grad_norm": 7.263438701629639,
      "learning_rate": 0.00019782135076252725,
      "loss": 2.979,
      "step": 50
    },
    {
      "epoch": 0.13071895424836602,
      "grad_norm": 7.077216148376465,
      "learning_rate": 0.0001973856209150327,
      "loss": 2.8306,
      "step": 60
    },
    {
      "epoch": 0.15250544662309368,
      "grad_norm": 7.652536869049072,
      "learning_rate": 0.00019694989106753814,
      "loss": 2.7815,
      "step": 70
    },
    {
      "epoch": 0.17429193899782136,
      "grad_norm": 6.8749165534973145,
      "learning_rate": 0.0001965141612200436,
      "loss": 2.6701,
      "step": 80
    },
    {
      "epoch": 0.19607843137254902,
      "grad_norm": 7.877785682678223,
      "learning_rate": 0.000196078431372549,
      "loss": 2.5733,
      "step": 90
    },
    {
      "epoch": 0.2178649237472767,
      "grad_norm": 8.357208251953125,
      "learning_rate": 0.00019564270152505449,
      "loss": 2.5055,
      "step": 100
    },
    {
      "epoch": 0.23965141612200436,
      "grad_norm": 8.622899055480957,
      "learning_rate": 0.00019520697167755993,
      "loss": 2.4145,
      "step": 110
    },
    {
      "epoch": 0.26143790849673204,
      "grad_norm": 6.556448936462402,
      "learning_rate": 0.00019477124183006535,
      "loss": 2.2737,
      "step": 120
    },
    {
      "epoch": 0.28322440087145967,
      "grad_norm": 7.372683048248291,
      "learning_rate": 0.0001943355119825708,
      "loss": 2.2912,
      "step": 130
    },
    {
      "epoch": 0.30501089324618735,
      "grad_norm": 7.385293006896973,
      "learning_rate": 0.00019389978213507628,
      "loss": 2.1606,
      "step": 140
    },
    {
      "epoch": 0.32679738562091504,
      "grad_norm": 6.5655388832092285,
      "learning_rate": 0.0001934640522875817,
      "loss": 2.1072,
      "step": 150
    },
    {
      "epoch": 0.3485838779956427,
      "grad_norm": 5.942952632904053,
      "learning_rate": 0.00019302832244008715,
      "loss": 1.9941,
      "step": 160
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 8.557580947875977,
      "learning_rate": 0.0001925925925925926,
      "loss": 1.9563,
      "step": 170
    },
    {
      "epoch": 0.39215686274509803,
      "grad_norm": 7.6008124351501465,
      "learning_rate": 0.00019215686274509807,
      "loss": 1.9714,
      "step": 180
    },
    {
      "epoch": 0.4139433551198257,
      "grad_norm": 8.176041603088379,
      "learning_rate": 0.0001917211328976035,
      "loss": 2.007,
      "step": 190
    },
    {
      "epoch": 0.4357298474945534,
      "grad_norm": 8.303592681884766,
      "learning_rate": 0.00019128540305010894,
      "loss": 1.9992,
      "step": 200
    },
    {
      "epoch": 0.45751633986928103,
      "grad_norm": 9.323390007019043,
      "learning_rate": 0.0001908496732026144,
      "loss": 1.8472,
      "step": 210
    },
    {
      "epoch": 0.4793028322440087,
      "grad_norm": 5.600342750549316,
      "learning_rate": 0.00019041394335511983,
      "loss": 1.5964,
      "step": 220
    },
    {
      "epoch": 0.5010893246187363,
      "grad_norm": 7.764472007751465,
      "learning_rate": 0.00018997821350762528,
      "loss": 1.5362,
      "step": 230
    },
    {
      "epoch": 0.5228758169934641,
      "grad_norm": 5.7999267578125,
      "learning_rate": 0.00018954248366013073,
      "loss": 1.7057,
      "step": 240
    },
    {
      "epoch": 0.5446623093681917,
      "grad_norm": 7.236270904541016,
      "learning_rate": 0.00018910675381263617,
      "loss": 1.5651,
      "step": 250
    },
    {
      "epoch": 0.5664488017429193,
      "grad_norm": 6.2355217933654785,
      "learning_rate": 0.00018867102396514162,
      "loss": 1.434,
      "step": 260
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 7.621241569519043,
      "learning_rate": 0.00018823529411764707,
      "loss": 1.4513,
      "step": 270
    },
    {
      "epoch": 0.6100217864923747,
      "grad_norm": 9.239877700805664,
      "learning_rate": 0.00018779956427015252,
      "loss": 1.4832,
      "step": 280
    },
    {
      "epoch": 0.6318082788671024,
      "grad_norm": 6.318016052246094,
      "learning_rate": 0.00018736383442265796,
      "loss": 1.5537,
      "step": 290
    },
    {
      "epoch": 0.6535947712418301,
      "grad_norm": 7.854848861694336,
      "learning_rate": 0.0001869281045751634,
      "loss": 1.5401,
      "step": 300
    },
    {
      "epoch": 0.6753812636165577,
      "grad_norm": 6.564336776733398,
      "learning_rate": 0.00018649237472766886,
      "loss": 1.5555,
      "step": 310
    },
    {
      "epoch": 0.6971677559912854,
      "grad_norm": 7.222121715545654,
      "learning_rate": 0.00018605664488017428,
      "loss": 1.3731,
      "step": 320
    },
    {
      "epoch": 0.7189542483660131,
      "grad_norm": 6.38707971572876,
      "learning_rate": 0.00018562091503267976,
      "loss": 1.5107,
      "step": 330
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 6.228954315185547,
      "learning_rate": 0.0001851851851851852,
      "loss": 1.3345,
      "step": 340
    },
    {
      "epoch": 0.7625272331154684,
      "grad_norm": 7.235721588134766,
      "learning_rate": 0.00018474945533769062,
      "loss": 1.4025,
      "step": 350
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 6.43580961227417,
      "learning_rate": 0.00018431372549019607,
      "loss": 1.506,
      "step": 360
    },
    {
      "epoch": 0.8061002178649237,
      "grad_norm": 8.648673057556152,
      "learning_rate": 0.00018387799564270155,
      "loss": 1.5353,
      "step": 370
    },
    {
      "epoch": 0.8278867102396514,
      "grad_norm": 6.596764087677002,
      "learning_rate": 0.00018344226579520697,
      "loss": 1.2951,
      "step": 380
    },
    {
      "epoch": 0.8496732026143791,
      "grad_norm": 6.486104965209961,
      "learning_rate": 0.00018300653594771241,
      "loss": 1.3391,
      "step": 390
    },
    {
      "epoch": 0.8714596949891068,
      "grad_norm": 7.029482841491699,
      "learning_rate": 0.0001825708061002179,
      "loss": 1.3067,
      "step": 400
    },
    {
      "epoch": 0.8932461873638344,
      "grad_norm": 6.523773670196533,
      "learning_rate": 0.00018213507625272334,
      "loss": 1.3845,
      "step": 410
    },
    {
      "epoch": 0.9150326797385621,
      "grad_norm": 8.478684425354004,
      "learning_rate": 0.00018169934640522876,
      "loss": 1.2939,
      "step": 420
    },
    {
      "epoch": 0.9368191721132898,
      "grad_norm": 9.096922874450684,
      "learning_rate": 0.0001812636165577342,
      "loss": 1.2864,
      "step": 430
    },
    {
      "epoch": 0.9586056644880174,
      "grad_norm": 8.667677879333496,
      "learning_rate": 0.00018082788671023968,
      "loss": 1.2807,
      "step": 440
    },
    {
      "epoch": 0.9803921568627451,
      "grad_norm": 6.484628200531006,
      "learning_rate": 0.0001803921568627451,
      "loss": 1.2984,
      "step": 450
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7213740458015268,
      "eval_loss": 0.9532918930053711,
      "eval_runtime": 18.7957,
      "eval_samples_per_second": 97.575,
      "eval_steps_per_second": 12.237,
      "step": 459
    },
    {
      "epoch": 1.0021786492374727,
      "grad_norm": 7.181089878082275,
      "learning_rate": 0.00017995642701525055,
      "loss": 1.4029,
      "step": 460
    },
    {
      "epoch": 1.0239651416122004,
      "grad_norm": 5.962271690368652,
      "learning_rate": 0.000179520697167756,
      "loss": 1.2818,
      "step": 470
    },
    {
      "epoch": 1.0457516339869282,
      "grad_norm": 4.7569780349731445,
      "learning_rate": 0.00017908496732026144,
      "loss": 1.0902,
      "step": 480
    },
    {
      "epoch": 1.0675381263616557,
      "grad_norm": 5.477270126342773,
      "learning_rate": 0.0001786492374727669,
      "loss": 1.2285,
      "step": 490
    },
    {
      "epoch": 1.0893246187363834,
      "grad_norm": 5.792172431945801,
      "learning_rate": 0.00017821350762527234,
      "loss": 1.1586,
      "step": 500
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 7.3204827308654785,
      "learning_rate": 0.00017777777777777779,
      "loss": 1.1715,
      "step": 510
    },
    {
      "epoch": 1.132897603485839,
      "grad_norm": 10.443838119506836,
      "learning_rate": 0.00017734204793028323,
      "loss": 1.1989,
      "step": 520
    },
    {
      "epoch": 1.1546840958605664,
      "grad_norm": 7.495549201965332,
      "learning_rate": 0.00017690631808278868,
      "loss": 1.0561,
      "step": 530
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 8.278329849243164,
      "learning_rate": 0.00017647058823529413,
      "loss": 1.1104,
      "step": 540
    },
    {
      "epoch": 1.1982570806100217,
      "grad_norm": 8.7997465133667,
      "learning_rate": 0.00017603485838779955,
      "loss": 0.991,
      "step": 550
    },
    {
      "epoch": 1.2200435729847494,
      "grad_norm": 6.62467098236084,
      "learning_rate": 0.00017559912854030502,
      "loss": 1.0119,
      "step": 560
    },
    {
      "epoch": 1.2418300653594772,
      "grad_norm": 8.42740535736084,
      "learning_rate": 0.00017516339869281047,
      "loss": 1.0973,
      "step": 570
    },
    {
      "epoch": 1.263616557734205,
      "grad_norm": 5.33419942855835,
      "learning_rate": 0.0001747276688453159,
      "loss": 1.188,
      "step": 580
    },
    {
      "epoch": 1.2854030501089324,
      "grad_norm": 10.315900802612305,
      "learning_rate": 0.00017429193899782137,
      "loss": 1.0946,
      "step": 590
    },
    {
      "epoch": 1.3071895424836601,
      "grad_norm": 8.559635162353516,
      "learning_rate": 0.00017385620915032682,
      "loss": 1.0699,
      "step": 600
    },
    {
      "epoch": 1.3289760348583877,
      "grad_norm": 6.452418327331543,
      "learning_rate": 0.00017342047930283226,
      "loss": 1.0903,
      "step": 610
    },
    {
      "epoch": 1.3507625272331154,
      "grad_norm": 7.335483551025391,
      "learning_rate": 0.00017298474945533768,
      "loss": 1.007,
      "step": 620
    },
    {
      "epoch": 1.3725490196078431,
      "grad_norm": 8.736480712890625,
      "learning_rate": 0.00017254901960784316,
      "loss": 0.9823,
      "step": 630
    },
    {
      "epoch": 1.3943355119825709,
      "grad_norm": 8.5095796585083,
      "learning_rate": 0.0001721132897603486,
      "loss": 1.1965,
      "step": 640
    },
    {
      "epoch": 1.4161220043572984,
      "grad_norm": 7.300091743469238,
      "learning_rate": 0.00017167755991285403,
      "loss": 1.0818,
      "step": 650
    },
    {
      "epoch": 1.4379084967320261,
      "grad_norm": 8.229924201965332,
      "learning_rate": 0.00017124183006535947,
      "loss": 0.9131,
      "step": 660
    },
    {
      "epoch": 1.4596949891067539,
      "grad_norm": 6.705434799194336,
      "learning_rate": 0.00017080610021786495,
      "loss": 1.1098,
      "step": 670
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 6.66180944442749,
      "learning_rate": 0.00017037037037037037,
      "loss": 1.0075,
      "step": 680
    },
    {
      "epoch": 1.5032679738562091,
      "grad_norm": 4.563886642456055,
      "learning_rate": 0.00016993464052287582,
      "loss": 1.2223,
      "step": 690
    },
    {
      "epoch": 1.5250544662309369,
      "grad_norm": 5.85321569442749,
      "learning_rate": 0.00016949891067538126,
      "loss": 0.9373,
      "step": 700
    },
    {
      "epoch": 1.5468409586056646,
      "grad_norm": 6.679529190063477,
      "learning_rate": 0.0001690631808278867,
      "loss": 1.1278,
      "step": 710
    },
    {
      "epoch": 1.5686274509803921,
      "grad_norm": 6.9362897872924805,
      "learning_rate": 0.00016862745098039216,
      "loss": 1.0249,
      "step": 720
    },
    {
      "epoch": 1.5904139433551199,
      "grad_norm": 9.71500015258789,
      "learning_rate": 0.0001681917211328976,
      "loss": 1.1462,
      "step": 730
    },
    {
      "epoch": 1.6122004357298474,
      "grad_norm": 9.561461448669434,
      "learning_rate": 0.00016775599128540308,
      "loss": 1.0879,
      "step": 740
    },
    {
      "epoch": 1.6339869281045751,
      "grad_norm": 9.36153793334961,
      "learning_rate": 0.0001673202614379085,
      "loss": 1.0274,
      "step": 750
    },
    {
      "epoch": 1.6557734204793029,
      "grad_norm": 6.499135494232178,
      "learning_rate": 0.00016688453159041395,
      "loss": 1.0688,
      "step": 760
    },
    {
      "epoch": 1.6775599128540306,
      "grad_norm": 5.580620765686035,
      "learning_rate": 0.0001664488017429194,
      "loss": 1.1125,
      "step": 770
    },
    {
      "epoch": 1.6993464052287581,
      "grad_norm": 5.770435333251953,
      "learning_rate": 0.00016601307189542485,
      "loss": 1.0711,
      "step": 780
    },
    {
      "epoch": 1.7211328976034859,
      "grad_norm": 7.610073089599609,
      "learning_rate": 0.0001655773420479303,
      "loss": 1.0684,
      "step": 790
    },
    {
      "epoch": 1.7429193899782134,
      "grad_norm": 10.024738311767578,
      "learning_rate": 0.00016514161220043574,
      "loss": 0.9415,
      "step": 800
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 9.421009063720703,
      "learning_rate": 0.0001647058823529412,
      "loss": 1.0256,
      "step": 810
    },
    {
      "epoch": 1.7864923747276689,
      "grad_norm": 8.817363739013672,
      "learning_rate": 0.00016427015250544664,
      "loss": 0.9626,
      "step": 820
    },
    {
      "epoch": 1.8082788671023966,
      "grad_norm": 7.055505275726318,
      "learning_rate": 0.00016383442265795208,
      "loss": 0.8611,
      "step": 830
    },
    {
      "epoch": 1.8300653594771243,
      "grad_norm": 8.723076820373535,
      "learning_rate": 0.00016339869281045753,
      "loss": 1.0573,
      "step": 840
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 6.406070709228516,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.9404,
      "step": 850
    },
    {
      "epoch": 1.8736383442265794,
      "grad_norm": 7.548007488250732,
      "learning_rate": 0.00016252723311546843,
      "loss": 0.9814,
      "step": 860
    },
    {
      "epoch": 1.8954248366013071,
      "grad_norm": 9.202605247497559,
      "learning_rate": 0.00016209150326797388,
      "loss": 0.9435,
      "step": 870
    },
    {
      "epoch": 1.9172113289760349,
      "grad_norm": 7.064706802368164,
      "learning_rate": 0.0001616557734204793,
      "loss": 0.8953,
      "step": 880
    },
    {
      "epoch": 1.9389978213507626,
      "grad_norm": 8.095579147338867,
      "learning_rate": 0.00016122004357298474,
      "loss": 0.9236,
      "step": 890
    },
    {
      "epoch": 1.9607843137254903,
      "grad_norm": 5.83788537979126,
      "learning_rate": 0.00016078431372549022,
      "loss": 0.9085,
      "step": 900
    },
    {
      "epoch": 1.9825708061002179,
      "grad_norm": 7.811189651489258,
      "learning_rate": 0.00016034858387799564,
      "loss": 1.0066,
      "step": 910
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.787350054525627,
      "eval_loss": 0.7130476832389832,
      "eval_runtime": 17.9488,
      "eval_samples_per_second": 102.179,
      "eval_steps_per_second": 12.814,
      "step": 918
    },
    {
      "epoch": 2.0043572984749454,
      "grad_norm": 6.506624698638916,
      "learning_rate": 0.00015991285403050109,
      "loss": 0.9322,
      "step": 920
    },
    {
      "epoch": 2.026143790849673,
      "grad_norm": 9.598650932312012,
      "learning_rate": 0.00015947712418300656,
      "loss": 0.9975,
      "step": 930
    },
    {
      "epoch": 2.047930283224401,
      "grad_norm": 8.938118934631348,
      "learning_rate": 0.00015904139433551198,
      "loss": 0.7595,
      "step": 940
    },
    {
      "epoch": 2.0697167755991286,
      "grad_norm": 9.58970832824707,
      "learning_rate": 0.00015860566448801743,
      "loss": 1.1068,
      "step": 950
    },
    {
      "epoch": 2.0915032679738563,
      "grad_norm": 8.663003921508789,
      "learning_rate": 0.00015816993464052288,
      "loss": 0.9472,
      "step": 960
    },
    {
      "epoch": 2.113289760348584,
      "grad_norm": 6.894487380981445,
      "learning_rate": 0.00015773420479302835,
      "loss": 0.9716,
      "step": 970
    },
    {
      "epoch": 2.1350762527233114,
      "grad_norm": 7.42901086807251,
      "learning_rate": 0.00015729847494553377,
      "loss": 0.7724,
      "step": 980
    },
    {
      "epoch": 2.156862745098039,
      "grad_norm": 6.131285190582275,
      "learning_rate": 0.00015686274509803922,
      "loss": 0.8552,
      "step": 990
    },
    {
      "epoch": 2.178649237472767,
      "grad_norm": 6.010169506072998,
      "learning_rate": 0.00015642701525054467,
      "loss": 0.7723,
      "step": 1000
    },
    {
      "epoch": 2.2004357298474946,
      "grad_norm": 6.926283359527588,
      "learning_rate": 0.00015599128540305012,
      "loss": 0.7971,
      "step": 1010
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 6.633950233459473,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.8894,
      "step": 1020
    },
    {
      "epoch": 2.24400871459695,
      "grad_norm": 5.015465259552002,
      "learning_rate": 0.000155119825708061,
      "loss": 0.8165,
      "step": 1030
    },
    {
      "epoch": 2.265795206971678,
      "grad_norm": 4.356743335723877,
      "learning_rate": 0.00015468409586056646,
      "loss": 0.856,
      "step": 1040
    },
    {
      "epoch": 2.287581699346405,
      "grad_norm": 5.18652868270874,
      "learning_rate": 0.0001542483660130719,
      "loss": 0.8071,
      "step": 1050
    },
    {
      "epoch": 2.309368191721133,
      "grad_norm": 10.0142183303833,
      "learning_rate": 0.00015381263616557735,
      "loss": 0.8395,
      "step": 1060
    },
    {
      "epoch": 2.3311546840958606,
      "grad_norm": 5.993680953979492,
      "learning_rate": 0.0001533769063180828,
      "loss": 0.7596,
      "step": 1070
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 7.268001556396484,
      "learning_rate": 0.00015294117647058822,
      "loss": 0.8751,
      "step": 1080
    },
    {
      "epoch": 2.374727668845316,
      "grad_norm": 4.8629984855651855,
      "learning_rate": 0.0001525054466230937,
      "loss": 0.8671,
      "step": 1090
    },
    {
      "epoch": 2.3965141612200433,
      "grad_norm": 7.185265064239502,
      "learning_rate": 0.00015206971677559914,
      "loss": 0.967,
      "step": 1100
    },
    {
      "epoch": 2.418300653594771,
      "grad_norm": 6.172699451446533,
      "learning_rate": 0.00015163398692810456,
      "loss": 0.824,
      "step": 1110
    },
    {
      "epoch": 2.440087145969499,
      "grad_norm": 9.820575714111328,
      "learning_rate": 0.00015119825708061004,
      "loss": 0.8161,
      "step": 1120
    },
    {
      "epoch": 2.4618736383442266,
      "grad_norm": 5.659167289733887,
      "learning_rate": 0.0001507625272331155,
      "loss": 0.8518,
      "step": 1130
    },
    {
      "epoch": 2.4836601307189543,
      "grad_norm": 5.147099018096924,
      "learning_rate": 0.0001503267973856209,
      "loss": 0.8873,
      "step": 1140
    },
    {
      "epoch": 2.505446623093682,
      "grad_norm": 5.560391902923584,
      "learning_rate": 0.00014989106753812636,
      "loss": 0.9059,
      "step": 1150
    },
    {
      "epoch": 2.52723311546841,
      "grad_norm": 8.400500297546387,
      "learning_rate": 0.00014945533769063183,
      "loss": 0.9281,
      "step": 1160
    },
    {
      "epoch": 2.549019607843137,
      "grad_norm": 4.928888320922852,
      "learning_rate": 0.00014901960784313728,
      "loss": 0.9415,
      "step": 1170
    },
    {
      "epoch": 2.570806100217865,
      "grad_norm": 8.203706741333008,
      "learning_rate": 0.0001485838779956427,
      "loss": 0.7277,
      "step": 1180
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 6.169627666473389,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.7953,
      "step": 1190
    },
    {
      "epoch": 2.6143790849673203,
      "grad_norm": 10.336771965026855,
      "learning_rate": 0.00014771241830065362,
      "loss": 1.0383,
      "step": 1200
    },
    {
      "epoch": 2.636165577342048,
      "grad_norm": 5.812628269195557,
      "learning_rate": 0.00014727668845315904,
      "loss": 0.8079,
      "step": 1210
    },
    {
      "epoch": 2.6579520697167753,
      "grad_norm": 7.741353988647461,
      "learning_rate": 0.0001468409586056645,
      "loss": 0.9655,
      "step": 1220
    },
    {
      "epoch": 2.6797385620915035,
      "grad_norm": 5.582716464996338,
      "learning_rate": 0.00014640522875816994,
      "loss": 0.8021,
      "step": 1230
    },
    {
      "epoch": 2.701525054466231,
      "grad_norm": 6.38383674621582,
      "learning_rate": 0.00014596949891067538,
      "loss": 0.9304,
      "step": 1240
    },
    {
      "epoch": 2.7233115468409586,
      "grad_norm": 7.046319484710693,
      "learning_rate": 0.00014553376906318083,
      "loss": 1.0827,
      "step": 1250
    },
    {
      "epoch": 2.7450980392156863,
      "grad_norm": 8.694887161254883,
      "learning_rate": 0.00014509803921568628,
      "loss": 0.9619,
      "step": 1260
    },
    {
      "epoch": 2.766884531590414,
      "grad_norm": 7.445662975311279,
      "learning_rate": 0.00014466230936819173,
      "loss": 0.8925,
      "step": 1270
    },
    {
      "epoch": 2.7886710239651418,
      "grad_norm": 6.552379608154297,
      "learning_rate": 0.00014422657952069718,
      "loss": 0.7242,
      "step": 1280
    },
    {
      "epoch": 2.810457516339869,
      "grad_norm": 6.047237873077393,
      "learning_rate": 0.00014379084967320262,
      "loss": 0.923,
      "step": 1290
    },
    {
      "epoch": 2.832244008714597,
      "grad_norm": 6.370119094848633,
      "learning_rate": 0.00014335511982570807,
      "loss": 0.8012,
      "step": 1300
    },
    {
      "epoch": 2.8540305010893245,
      "grad_norm": 9.1171875,
      "learning_rate": 0.00014291938997821352,
      "loss": 0.9152,
      "step": 1310
    },
    {
      "epoch": 2.8758169934640523,
      "grad_norm": 6.716188907623291,
      "learning_rate": 0.00014248366013071897,
      "loss": 0.8193,
      "step": 1320
    },
    {
      "epoch": 2.89760348583878,
      "grad_norm": 7.116605758666992,
      "learning_rate": 0.0001420479302832244,
      "loss": 0.9454,
      "step": 1330
    },
    {
      "epoch": 2.9193899782135078,
      "grad_norm": 4.328480243682861,
      "learning_rate": 0.00014161220043572983,
      "loss": 0.8427,
      "step": 1340
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 8.35233211517334,
      "learning_rate": 0.0001411764705882353,
      "loss": 0.9693,
      "step": 1350
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 6.970639705657959,
      "learning_rate": 0.00014074074074074076,
      "loss": 1.0024,
      "step": 1360
    },
    {
      "epoch": 2.9847494553376905,
      "grad_norm": 6.220310211181641,
      "learning_rate": 0.0001403050108932462,
      "loss": 0.7182,
      "step": 1370
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.816793893129771,
      "eval_loss": 0.6386911869049072,
      "eval_runtime": 17.9899,
      "eval_samples_per_second": 101.946,
      "eval_steps_per_second": 12.785,
      "step": 1377
    },
    {
      "epoch": 3.0065359477124183,
      "grad_norm": 6.348875999450684,
      "learning_rate": 0.00013986928104575162,
      "loss": 0.6622,
      "step": 1380
    },
    {
      "epoch": 3.028322440087146,
      "grad_norm": 8.997797966003418,
      "learning_rate": 0.0001394335511982571,
      "loss": 0.8994,
      "step": 1390
    },
    {
      "epoch": 3.0501089324618738,
      "grad_norm": 2.507059335708618,
      "learning_rate": 0.00013899782135076255,
      "loss": 0.667,
      "step": 1400
    },
    {
      "epoch": 3.0718954248366015,
      "grad_norm": 6.727390289306641,
      "learning_rate": 0.00013856209150326797,
      "loss": 0.7828,
      "step": 1410
    },
    {
      "epoch": 3.093681917211329,
      "grad_norm": 6.624542236328125,
      "learning_rate": 0.00013812636165577342,
      "loss": 0.7444,
      "step": 1420
    },
    {
      "epoch": 3.1154684095860565,
      "grad_norm": 4.031960487365723,
      "learning_rate": 0.0001376906318082789,
      "loss": 0.6866,
      "step": 1430
    },
    {
      "epoch": 3.1372549019607843,
      "grad_norm": 5.771276473999023,
      "learning_rate": 0.0001372549019607843,
      "loss": 0.9353,
      "step": 1440
    },
    {
      "epoch": 3.159041394335512,
      "grad_norm": 5.536561012268066,
      "learning_rate": 0.00013681917211328976,
      "loss": 0.7841,
      "step": 1450
    },
    {
      "epoch": 3.1808278867102397,
      "grad_norm": 7.103678226470947,
      "learning_rate": 0.0001363834422657952,
      "loss": 0.6723,
      "step": 1460
    },
    {
      "epoch": 3.2026143790849675,
      "grad_norm": 8.411959648132324,
      "learning_rate": 0.00013594771241830065,
      "loss": 0.6801,
      "step": 1470
    },
    {
      "epoch": 3.224400871459695,
      "grad_norm": 4.662508010864258,
      "learning_rate": 0.0001355119825708061,
      "loss": 0.6556,
      "step": 1480
    },
    {
      "epoch": 3.2461873638344225,
      "grad_norm": 9.703165054321289,
      "learning_rate": 0.00013507625272331155,
      "loss": 0.8528,
      "step": 1490
    },
    {
      "epoch": 3.2679738562091503,
      "grad_norm": 11.6607084274292,
      "learning_rate": 0.000134640522875817,
      "loss": 0.7218,
      "step": 1500
    },
    {
      "epoch": 3.289760348583878,
      "grad_norm": 7.47981071472168,
      "learning_rate": 0.00013420479302832244,
      "loss": 0.7278,
      "step": 1510
    },
    {
      "epoch": 3.3115468409586057,
      "grad_norm": 7.397233009338379,
      "learning_rate": 0.0001337690631808279,
      "loss": 0.7781,
      "step": 1520
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 8.832053184509277,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.753,
      "step": 1530
    },
    {
      "epoch": 3.355119825708061,
      "grad_norm": 9.308136940002441,
      "learning_rate": 0.0001328976034858388,
      "loss": 0.8678,
      "step": 1540
    },
    {
      "epoch": 3.3769063180827885,
      "grad_norm": 10.303658485412598,
      "learning_rate": 0.00013246187363834424,
      "loss": 0.798,
      "step": 1550
    },
    {
      "epoch": 3.3986928104575163,
      "grad_norm": 6.4591965675354,
      "learning_rate": 0.00013202614379084968,
      "loss": 0.8441,
      "step": 1560
    },
    {
      "epoch": 3.420479302832244,
      "grad_norm": 6.536279201507568,
      "learning_rate": 0.00013159041394335513,
      "loss": 0.7794,
      "step": 1570
    },
    {
      "epoch": 3.4422657952069717,
      "grad_norm": 6.244722366333008,
      "learning_rate": 0.00013115468409586058,
      "loss": 0.6543,
      "step": 1580
    },
    {
      "epoch": 3.4640522875816995,
      "grad_norm": 7.815539836883545,
      "learning_rate": 0.00013071895424836603,
      "loss": 0.6514,
      "step": 1590
    },
    {
      "epoch": 3.4858387799564268,
      "grad_norm": 6.649170398712158,
      "learning_rate": 0.00013028322440087147,
      "loss": 0.6915,
      "step": 1600
    },
    {
      "epoch": 3.507625272331155,
      "grad_norm": 7.287216663360596,
      "learning_rate": 0.0001298474945533769,
      "loss": 0.7718,
      "step": 1610
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 7.989932060241699,
      "learning_rate": 0.00012941176470588237,
      "loss": 0.9292,
      "step": 1620
    },
    {
      "epoch": 3.55119825708061,
      "grad_norm": 8.176399230957031,
      "learning_rate": 0.00012897603485838782,
      "loss": 0.8365,
      "step": 1630
    },
    {
      "epoch": 3.5729847494553377,
      "grad_norm": 7.736656665802002,
      "learning_rate": 0.00012854030501089324,
      "loss": 0.7914,
      "step": 1640
    },
    {
      "epoch": 3.5947712418300655,
      "grad_norm": 5.909413814544678,
      "learning_rate": 0.00012810457516339868,
      "loss": 0.7402,
      "step": 1650
    },
    {
      "epoch": 3.616557734204793,
      "grad_norm": 6.814175128936768,
      "learning_rate": 0.00012766884531590416,
      "loss": 0.7247,
      "step": 1660
    },
    {
      "epoch": 3.6383442265795205,
      "grad_norm": 8.350125312805176,
      "learning_rate": 0.00012723311546840958,
      "loss": 0.7829,
      "step": 1670
    },
    {
      "epoch": 3.6601307189542482,
      "grad_norm": 9.213706016540527,
      "learning_rate": 0.00012679738562091503,
      "loss": 0.65,
      "step": 1680
    },
    {
      "epoch": 3.681917211328976,
      "grad_norm": 4.801314353942871,
      "learning_rate": 0.0001263616557734205,
      "loss": 0.6551,
      "step": 1690
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 7.545658588409424,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.7394,
      "step": 1700
    },
    {
      "epoch": 3.7254901960784315,
      "grad_norm": 5.372350215911865,
      "learning_rate": 0.00012549019607843137,
      "loss": 0.7193,
      "step": 1710
    },
    {
      "epoch": 3.747276688453159,
      "grad_norm": 8.14167308807373,
      "learning_rate": 0.00012505446623093682,
      "loss": 0.8101,
      "step": 1720
    },
    {
      "epoch": 3.769063180827887,
      "grad_norm": 5.863802909851074,
      "learning_rate": 0.0001246187363834423,
      "loss": 0.5942,
      "step": 1730
    },
    {
      "epoch": 3.7908496732026142,
      "grad_norm": 5.3410563468933105,
      "learning_rate": 0.00012418300653594771,
      "loss": 0.6494,
      "step": 1740
    },
    {
      "epoch": 3.812636165577342,
      "grad_norm": 6.914513111114502,
      "learning_rate": 0.00012374727668845316,
      "loss": 0.7341,
      "step": 1750
    },
    {
      "epoch": 3.8344226579520697,
      "grad_norm": 8.532143592834473,
      "learning_rate": 0.0001233115468409586,
      "loss": 0.7964,
      "step": 1760
    },
    {
      "epoch": 3.8562091503267975,
      "grad_norm": 8.521903038024902,
      "learning_rate": 0.00012287581699346406,
      "loss": 0.7459,
      "step": 1770
    },
    {
      "epoch": 3.877995642701525,
      "grad_norm": 6.274909973144531,
      "learning_rate": 0.0001224400871459695,
      "loss": 0.6816,
      "step": 1780
    },
    {
      "epoch": 3.8997821350762525,
      "grad_norm": 7.667659282684326,
      "learning_rate": 0.00012200435729847495,
      "loss": 0.7404,
      "step": 1790
    },
    {
      "epoch": 3.9215686274509802,
      "grad_norm": 5.932393550872803,
      "learning_rate": 0.00012156862745098039,
      "loss": 0.677,
      "step": 1800
    },
    {
      "epoch": 3.943355119825708,
      "grad_norm": 7.391468524932861,
      "learning_rate": 0.00012113289760348585,
      "loss": 0.7004,
      "step": 1810
    },
    {
      "epoch": 3.9651416122004357,
      "grad_norm": 7.232853889465332,
      "learning_rate": 0.0001206971677559913,
      "loss": 0.8455,
      "step": 1820
    },
    {
      "epoch": 3.9869281045751634,
      "grad_norm": 6.784409523010254,
      "learning_rate": 0.00012026143790849673,
      "loss": 0.7269,
      "step": 1830
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8462377317339149,
      "eval_loss": 0.575934648513794,
      "eval_runtime": 18.1194,
      "eval_samples_per_second": 101.217,
      "eval_steps_per_second": 12.694,
      "step": 1836
    },
    {
      "epoch": 4.008714596949891,
      "grad_norm": 6.7415971755981445,
      "learning_rate": 0.00011982570806100219,
      "loss": 0.4561,
      "step": 1840
    },
    {
      "epoch": 4.030501089324619,
      "grad_norm": 9.656231880187988,
      "learning_rate": 0.00011938997821350764,
      "loss": 0.6967,
      "step": 1850
    },
    {
      "epoch": 4.052287581699346,
      "grad_norm": 6.1176323890686035,
      "learning_rate": 0.00011895424836601307,
      "loss": 0.5781,
      "step": 1860
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 7.1917219161987305,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.6001,
      "step": 1870
    },
    {
      "epoch": 4.095860566448802,
      "grad_norm": 6.089762210845947,
      "learning_rate": 0.00011808278867102398,
      "loss": 0.7777,
      "step": 1880
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 6.887173175811768,
      "learning_rate": 0.00011764705882352942,
      "loss": 0.6872,
      "step": 1890
    },
    {
      "epoch": 4.139433551198257,
      "grad_norm": 7.417105674743652,
      "learning_rate": 0.00011721132897603486,
      "loss": 0.5278,
      "step": 1900
    },
    {
      "epoch": 4.1612200435729845,
      "grad_norm": 6.751058101654053,
      "learning_rate": 0.0001167755991285403,
      "loss": 0.5604,
      "step": 1910
    },
    {
      "epoch": 4.183006535947713,
      "grad_norm": 8.985963821411133,
      "learning_rate": 0.00011633986928104577,
      "loss": 0.7086,
      "step": 1920
    },
    {
      "epoch": 4.20479302832244,
      "grad_norm": 5.215376377105713,
      "learning_rate": 0.0001159041394335512,
      "loss": 0.5659,
      "step": 1930
    },
    {
      "epoch": 4.226579520697168,
      "grad_norm": 5.371333599090576,
      "learning_rate": 0.00011546840958605665,
      "loss": 0.5222,
      "step": 1940
    },
    {
      "epoch": 4.248366013071895,
      "grad_norm": 6.072103500366211,
      "learning_rate": 0.00011503267973856209,
      "loss": 0.6825,
      "step": 1950
    },
    {
      "epoch": 4.270152505446623,
      "grad_norm": 9.212719917297363,
      "learning_rate": 0.00011459694989106755,
      "loss": 0.6149,
      "step": 1960
    },
    {
      "epoch": 4.291938997821351,
      "grad_norm": 6.528072357177734,
      "learning_rate": 0.000114161220043573,
      "loss": 0.563,
      "step": 1970
    },
    {
      "epoch": 4.313725490196078,
      "grad_norm": 4.879007816314697,
      "learning_rate": 0.00011372549019607843,
      "loss": 0.5461,
      "step": 1980
    },
    {
      "epoch": 4.335511982570806,
      "grad_norm": 6.021703720092773,
      "learning_rate": 0.00011328976034858388,
      "loss": 0.7482,
      "step": 1990
    },
    {
      "epoch": 4.357298474945534,
      "grad_norm": 4.678780555725098,
      "learning_rate": 0.00011285403050108934,
      "loss": 0.5772,
      "step": 2000
    },
    {
      "epoch": 4.379084967320262,
      "grad_norm": 7.869682788848877,
      "learning_rate": 0.00011241830065359477,
      "loss": 0.7251,
      "step": 2010
    },
    {
      "epoch": 4.400871459694989,
      "grad_norm": 9.97692584991455,
      "learning_rate": 0.00011202614379084968,
      "loss": 0.7899,
      "step": 2020
    },
    {
      "epoch": 4.4226579520697165,
      "grad_norm": 4.363010406494141,
      "learning_rate": 0.00011159041394335512,
      "loss": 0.5591,
      "step": 2030
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 6.863479137420654,
      "learning_rate": 0.00011115468409586057,
      "loss": 0.773,
      "step": 2040
    },
    {
      "epoch": 4.466230936819172,
      "grad_norm": 4.92185115814209,
      "learning_rate": 0.00011071895424836603,
      "loss": 0.5954,
      "step": 2050
    },
    {
      "epoch": 4.4880174291939,
      "grad_norm": 6.949173927307129,
      "learning_rate": 0.00011028322440087146,
      "loss": 0.6406,
      "step": 2060
    },
    {
      "epoch": 4.509803921568627,
      "grad_norm": 8.43314266204834,
      "learning_rate": 0.00010984749455337691,
      "loss": 0.6267,
      "step": 2070
    },
    {
      "epoch": 4.531590413943356,
      "grad_norm": 8.358415603637695,
      "learning_rate": 0.00010941176470588237,
      "loss": 0.8357,
      "step": 2080
    },
    {
      "epoch": 4.553376906318083,
      "grad_norm": 6.210323810577393,
      "learning_rate": 0.0001089760348583878,
      "loss": 0.7063,
      "step": 2090
    },
    {
      "epoch": 4.57516339869281,
      "grad_norm": 9.340721130371094,
      "learning_rate": 0.00010854030501089325,
      "loss": 0.6526,
      "step": 2100
    },
    {
      "epoch": 4.596949891067538,
      "grad_norm": 7.8128838539123535,
      "learning_rate": 0.00010810457516339869,
      "loss": 0.591,
      "step": 2110
    },
    {
      "epoch": 4.618736383442266,
      "grad_norm": 8.542261123657227,
      "learning_rate": 0.00010766884531590416,
      "loss": 0.6025,
      "step": 2120
    },
    {
      "epoch": 4.640522875816993,
      "grad_norm": 6.042516708374023,
      "learning_rate": 0.0001072331154684096,
      "loss": 0.6096,
      "step": 2130
    },
    {
      "epoch": 4.662309368191721,
      "grad_norm": 5.654211044311523,
      "learning_rate": 0.00010679738562091503,
      "loss": 0.6706,
      "step": 2140
    },
    {
      "epoch": 4.684095860566448,
      "grad_norm": 8.545449256896973,
      "learning_rate": 0.00010636165577342048,
      "loss": 0.7019,
      "step": 2150
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 7.199758529663086,
      "learning_rate": 0.00010592592592592594,
      "loss": 0.6676,
      "step": 2160
    },
    {
      "epoch": 4.727668845315904,
      "grad_norm": 9.946008682250977,
      "learning_rate": 0.00010549019607843139,
      "loss": 0.608,
      "step": 2170
    },
    {
      "epoch": 4.749455337690632,
      "grad_norm": 6.22027063369751,
      "learning_rate": 0.00010505446623093682,
      "loss": 0.8384,
      "step": 2180
    },
    {
      "epoch": 4.771241830065359,
      "grad_norm": 6.7131476402282715,
      "learning_rate": 0.00010461873638344227,
      "loss": 0.6487,
      "step": 2190
    },
    {
      "epoch": 4.793028322440087,
      "grad_norm": 1.7248843908309937,
      "learning_rate": 0.00010418300653594773,
      "loss": 0.5294,
      "step": 2200
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 8.164973258972168,
      "learning_rate": 0.00010374727668845316,
      "loss": 0.7439,
      "step": 2210
    },
    {
      "epoch": 4.836601307189542,
      "grad_norm": 5.255402565002441,
      "learning_rate": 0.00010331154684095861,
      "loss": 0.6281,
      "step": 2220
    },
    {
      "epoch": 4.85838779956427,
      "grad_norm": 10.198193550109863,
      "learning_rate": 0.00010287581699346404,
      "loss": 0.8054,
      "step": 2230
    },
    {
      "epoch": 4.880174291938998,
      "grad_norm": 5.84524393081665,
      "learning_rate": 0.0001024400871459695,
      "loss": 0.834,
      "step": 2240
    },
    {
      "epoch": 4.901960784313726,
      "grad_norm": 7.511988639831543,
      "learning_rate": 0.00010200435729847495,
      "loss": 0.5935,
      "step": 2250
    },
    {
      "epoch": 4.923747276688453,
      "grad_norm": 4.430392265319824,
      "learning_rate": 0.00010156862745098039,
      "loss": 0.6283,
      "step": 2260
    },
    {
      "epoch": 4.94553376906318,
      "grad_norm": 11.219193458557129,
      "learning_rate": 0.00010113289760348585,
      "loss": 0.6672,
      "step": 2270
    },
    {
      "epoch": 4.967320261437909,
      "grad_norm": 10.677290916442871,
      "learning_rate": 0.0001006971677559913,
      "loss": 0.6932,
      "step": 2280
    },
    {
      "epoch": 4.989106753812636,
      "grad_norm": 5.7056708335876465,
      "learning_rate": 0.00010026143790849673,
      "loss": 0.5371,
      "step": 2290
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8560523446019629,
      "eval_loss": 0.5136362910270691,
      "eval_runtime": 17.9552,
      "eval_samples_per_second": 102.143,
      "eval_steps_per_second": 12.81,
      "step": 2295
    },
    {
      "epoch": 5.010893246187364,
      "grad_norm": 6.218663692474365,
      "learning_rate": 9.982570806100219e-05,
      "loss": 0.5389,
      "step": 2300
    },
    {
      "epoch": 5.032679738562091,
      "grad_norm": 6.821216106414795,
      "learning_rate": 9.938997821350763e-05,
      "loss": 0.5182,
      "step": 2310
    },
    {
      "epoch": 5.05446623093682,
      "grad_norm": 7.251108169555664,
      "learning_rate": 9.895424836601307e-05,
      "loss": 0.6018,
      "step": 2320
    },
    {
      "epoch": 5.076252723311547,
      "grad_norm": 5.835667133331299,
      "learning_rate": 9.851851851851852e-05,
      "loss": 0.6159,
      "step": 2330
    },
    {
      "epoch": 5.098039215686274,
      "grad_norm": 2.2816996574401855,
      "learning_rate": 9.808278867102397e-05,
      "loss": 0.5066,
      "step": 2340
    },
    {
      "epoch": 5.119825708061002,
      "grad_norm": 6.499669075012207,
      "learning_rate": 9.764705882352942e-05,
      "loss": 0.5996,
      "step": 2350
    },
    {
      "epoch": 5.14161220043573,
      "grad_norm": 8.713186264038086,
      "learning_rate": 9.721132897603486e-05,
      "loss": 0.604,
      "step": 2360
    },
    {
      "epoch": 5.163398692810458,
      "grad_norm": 3.9944581985473633,
      "learning_rate": 9.677559912854031e-05,
      "loss": 0.5997,
      "step": 2370
    },
    {
      "epoch": 5.185185185185185,
      "grad_norm": 9.535465240478516,
      "learning_rate": 9.633986928104576e-05,
      "loss": 0.5218,
      "step": 2380
    },
    {
      "epoch": 5.206971677559913,
      "grad_norm": 5.211842060089111,
      "learning_rate": 9.59041394335512e-05,
      "loss": 0.627,
      "step": 2390
    },
    {
      "epoch": 5.228758169934641,
      "grad_norm": 3.8786020278930664,
      "learning_rate": 9.546840958605666e-05,
      "loss": 0.5076,
      "step": 2400
    },
    {
      "epoch": 5.250544662309368,
      "grad_norm": 5.325542449951172,
      "learning_rate": 9.503267973856209e-05,
      "loss": 0.6819,
      "step": 2410
    },
    {
      "epoch": 5.272331154684096,
      "grad_norm": 7.500797271728516,
      "learning_rate": 9.459694989106754e-05,
      "loss": 0.5258,
      "step": 2420
    },
    {
      "epoch": 5.294117647058823,
      "grad_norm": 7.870760440826416,
      "learning_rate": 9.416122004357298e-05,
      "loss": 0.4798,
      "step": 2430
    },
    {
      "epoch": 5.315904139433552,
      "grad_norm": 7.495830059051514,
      "learning_rate": 9.372549019607843e-05,
      "loss": 0.7252,
      "step": 2440
    },
    {
      "epoch": 5.337690631808279,
      "grad_norm": 6.7897233963012695,
      "learning_rate": 9.32897603485839e-05,
      "loss": 0.7106,
      "step": 2450
    },
    {
      "epoch": 5.359477124183006,
      "grad_norm": 6.39495849609375,
      "learning_rate": 9.285403050108933e-05,
      "loss": 0.6171,
      "step": 2460
    },
    {
      "epoch": 5.381263616557734,
      "grad_norm": 7.435427665710449,
      "learning_rate": 9.241830065359478e-05,
      "loss": 0.7785,
      "step": 2470
    },
    {
      "epoch": 5.403050108932462,
      "grad_norm": 3.6893563270568848,
      "learning_rate": 9.198257080610022e-05,
      "loss": 0.5686,
      "step": 2480
    },
    {
      "epoch": 5.42483660130719,
      "grad_norm": 5.704145431518555,
      "learning_rate": 9.154684095860567e-05,
      "loss": 0.4535,
      "step": 2490
    },
    {
      "epoch": 5.446623093681917,
      "grad_norm": 4.29396915435791,
      "learning_rate": 9.111111111111112e-05,
      "loss": 0.5368,
      "step": 2500
    },
    {
      "epoch": 5.468409586056645,
      "grad_norm": 5.111685752868652,
      "learning_rate": 9.067538126361657e-05,
      "loss": 0.4233,
      "step": 2510
    },
    {
      "epoch": 5.490196078431373,
      "grad_norm": 6.3384857177734375,
      "learning_rate": 9.0239651416122e-05,
      "loss": 0.4534,
      "step": 2520
    },
    {
      "epoch": 5.5119825708061,
      "grad_norm": 3.7532222270965576,
      "learning_rate": 8.980392156862746e-05,
      "loss": 0.557,
      "step": 2530
    },
    {
      "epoch": 5.533769063180828,
      "grad_norm": 7.596500873565674,
      "learning_rate": 8.93681917211329e-05,
      "loss": 0.4318,
      "step": 2540
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 7.933528423309326,
      "learning_rate": 8.893246187363836e-05,
      "loss": 0.4602,
      "step": 2550
    },
    {
      "epoch": 5.5773420479302835,
      "grad_norm": 9.436452865600586,
      "learning_rate": 8.849673202614379e-05,
      "loss": 0.5146,
      "step": 2560
    },
    {
      "epoch": 5.599128540305011,
      "grad_norm": 5.3257293701171875,
      "learning_rate": 8.806100217864924e-05,
      "loss": 0.6241,
      "step": 2570
    },
    {
      "epoch": 5.620915032679738,
      "grad_norm": 7.0885009765625,
      "learning_rate": 8.762527233115469e-05,
      "loss": 0.5623,
      "step": 2580
    },
    {
      "epoch": 5.642701525054466,
      "grad_norm": 5.873112201690674,
      "learning_rate": 8.718954248366013e-05,
      "loss": 0.5274,
      "step": 2590
    },
    {
      "epoch": 5.664488017429194,
      "grad_norm": 5.793452739715576,
      "learning_rate": 8.675381263616558e-05,
      "loss": 0.6477,
      "step": 2600
    },
    {
      "epoch": 5.686274509803922,
      "grad_norm": 4.69313383102417,
      "learning_rate": 8.631808278867103e-05,
      "loss": 0.5305,
      "step": 2610
    },
    {
      "epoch": 5.708061002178649,
      "grad_norm": 3.864166259765625,
      "learning_rate": 8.588235294117646e-05,
      "loss": 0.5066,
      "step": 2620
    },
    {
      "epoch": 5.729847494553377,
      "grad_norm": 4.964334011077881,
      "learning_rate": 8.544662309368192e-05,
      "loss": 0.5022,
      "step": 2630
    },
    {
      "epoch": 5.751633986928105,
      "grad_norm": 6.097200393676758,
      "learning_rate": 8.501089324618737e-05,
      "loss": 0.457,
      "step": 2640
    },
    {
      "epoch": 5.773420479302832,
      "grad_norm": 4.667783737182617,
      "learning_rate": 8.457516339869282e-05,
      "loss": 0.5466,
      "step": 2650
    },
    {
      "epoch": 5.79520697167756,
      "grad_norm": 3.8014779090881348,
      "learning_rate": 8.413943355119827e-05,
      "loss": 0.5643,
      "step": 2660
    },
    {
      "epoch": 5.816993464052287,
      "grad_norm": 4.136281490325928,
      "learning_rate": 8.37037037037037e-05,
      "loss": 0.7969,
      "step": 2670
    },
    {
      "epoch": 5.8387799564270155,
      "grad_norm": 8.202631950378418,
      "learning_rate": 8.326797385620916e-05,
      "loss": 0.5959,
      "step": 2680
    },
    {
      "epoch": 5.860566448801743,
      "grad_norm": 8.0128173828125,
      "learning_rate": 8.28322440087146e-05,
      "loss": 0.5908,
      "step": 2690
    },
    {
      "epoch": 5.882352941176471,
      "grad_norm": 5.717708587646484,
      "learning_rate": 8.239651416122004e-05,
      "loss": 0.4404,
      "step": 2700
    },
    {
      "epoch": 5.904139433551198,
      "grad_norm": 5.088473796844482,
      "learning_rate": 8.196078431372549e-05,
      "loss": 0.5249,
      "step": 2710
    },
    {
      "epoch": 5.925925925925926,
      "grad_norm": 6.980319499969482,
      "learning_rate": 8.152505446623094e-05,
      "loss": 0.5388,
      "step": 2720
    },
    {
      "epoch": 5.947712418300654,
      "grad_norm": 6.139514923095703,
      "learning_rate": 8.108932461873639e-05,
      "loss": 0.6152,
      "step": 2730
    },
    {
      "epoch": 5.969498910675381,
      "grad_norm": 11.050288200378418,
      "learning_rate": 8.065359477124184e-05,
      "loss": 0.7142,
      "step": 2740
    },
    {
      "epoch": 5.991285403050109,
      "grad_norm": 6.270197868347168,
      "learning_rate": 8.021786492374728e-05,
      "loss": 0.5696,
      "step": 2750
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.8685932388222465,
      "eval_loss": 0.4614640772342682,
      "eval_runtime": 17.9396,
      "eval_samples_per_second": 102.232,
      "eval_steps_per_second": 12.821,
      "step": 2754
    },
    {
      "epoch": 6.0130718954248366,
      "grad_norm": 3.350670337677002,
      "learning_rate": 7.978213507625273e-05,
      "loss": 0.5292,
      "step": 2760
    },
    {
      "epoch": 6.034858387799564,
      "grad_norm": 5.9170098304748535,
      "learning_rate": 7.934640522875816e-05,
      "loss": 0.4108,
      "step": 2770
    },
    {
      "epoch": 6.056644880174292,
      "grad_norm": 5.862520217895508,
      "learning_rate": 7.891067538126363e-05,
      "loss": 0.4113,
      "step": 2780
    },
    {
      "epoch": 6.078431372549019,
      "grad_norm": 5.952403545379639,
      "learning_rate": 7.847494553376906e-05,
      "loss": 0.5427,
      "step": 2790
    },
    {
      "epoch": 6.1002178649237475,
      "grad_norm": 5.758227825164795,
      "learning_rate": 7.803921568627451e-05,
      "loss": 0.496,
      "step": 2800
    },
    {
      "epoch": 6.122004357298475,
      "grad_norm": 5.548366546630859,
      "learning_rate": 7.760348583877996e-05,
      "loss": 0.5779,
      "step": 2810
    },
    {
      "epoch": 6.143790849673203,
      "grad_norm": 3.8540830612182617,
      "learning_rate": 7.71677559912854e-05,
      "loss": 0.6327,
      "step": 2820
    },
    {
      "epoch": 6.16557734204793,
      "grad_norm": 8.90394401550293,
      "learning_rate": 7.673202614379086e-05,
      "loss": 0.615,
      "step": 2830
    },
    {
      "epoch": 6.187363834422658,
      "grad_norm": 6.255537033081055,
      "learning_rate": 7.62962962962963e-05,
      "loss": 0.5795,
      "step": 2840
    },
    {
      "epoch": 6.209150326797386,
      "grad_norm": 3.820336103439331,
      "learning_rate": 7.586056644880175e-05,
      "loss": 0.4791,
      "step": 2850
    },
    {
      "epoch": 6.230936819172113,
      "grad_norm": 9.299781799316406,
      "learning_rate": 7.54248366013072e-05,
      "loss": 0.5079,
      "step": 2860
    },
    {
      "epoch": 6.252723311546841,
      "grad_norm": 9.52072525024414,
      "learning_rate": 7.498910675381264e-05,
      "loss": 0.39,
      "step": 2870
    },
    {
      "epoch": 6.2745098039215685,
      "grad_norm": 7.669307708740234,
      "learning_rate": 7.455337690631809e-05,
      "loss": 0.4475,
      "step": 2880
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 9.115036964416504,
      "learning_rate": 7.411764705882354e-05,
      "loss": 0.4291,
      "step": 2890
    },
    {
      "epoch": 6.318082788671024,
      "grad_norm": 5.864743709564209,
      "learning_rate": 7.368191721132897e-05,
      "loss": 0.5047,
      "step": 2900
    },
    {
      "epoch": 6.339869281045751,
      "grad_norm": 5.831567287445068,
      "learning_rate": 7.324618736383443e-05,
      "loss": 0.426,
      "step": 2910
    },
    {
      "epoch": 6.3616557734204795,
      "grad_norm": 6.850208282470703,
      "learning_rate": 7.281045751633987e-05,
      "loss": 0.4024,
      "step": 2920
    },
    {
      "epoch": 6.383442265795207,
      "grad_norm": 7.724635124206543,
      "learning_rate": 7.237472766884533e-05,
      "loss": 0.4072,
      "step": 2930
    },
    {
      "epoch": 6.405228758169935,
      "grad_norm": 5.775803089141846,
      "learning_rate": 7.193899782135076e-05,
      "loss": 0.4741,
      "step": 2940
    },
    {
      "epoch": 6.427015250544662,
      "grad_norm": 8.415946960449219,
      "learning_rate": 7.150326797385621e-05,
      "loss": 0.5635,
      "step": 2950
    },
    {
      "epoch": 6.44880174291939,
      "grad_norm": 5.6876654624938965,
      "learning_rate": 7.106753812636166e-05,
      "loss": 0.4189,
      "step": 2960
    },
    {
      "epoch": 6.470588235294118,
      "grad_norm": 9.38522720336914,
      "learning_rate": 7.06318082788671e-05,
      "loss": 0.4635,
      "step": 2970
    },
    {
      "epoch": 6.492374727668845,
      "grad_norm": 8.251582145690918,
      "learning_rate": 7.019607843137255e-05,
      "loss": 0.6176,
      "step": 2980
    },
    {
      "epoch": 6.514161220043573,
      "grad_norm": 4.9220662117004395,
      "learning_rate": 6.9760348583878e-05,
      "loss": 0.5474,
      "step": 2990
    },
    {
      "epoch": 6.5359477124183005,
      "grad_norm": 6.880181789398193,
      "learning_rate": 6.932461873638345e-05,
      "loss": 0.5613,
      "step": 3000
    },
    {
      "epoch": 6.557734204793029,
      "grad_norm": 5.987041473388672,
      "learning_rate": 6.88888888888889e-05,
      "loss": 0.4076,
      "step": 3010
    },
    {
      "epoch": 6.579520697167756,
      "grad_norm": 5.497262477874756,
      "learning_rate": 6.845315904139434e-05,
      "loss": 0.4229,
      "step": 3020
    },
    {
      "epoch": 6.601307189542483,
      "grad_norm": 4.298800468444824,
      "learning_rate": 6.801742919389979e-05,
      "loss": 0.4229,
      "step": 3030
    },
    {
      "epoch": 6.6230936819172115,
      "grad_norm": 7.030991077423096,
      "learning_rate": 6.758169934640524e-05,
      "loss": 0.6026,
      "step": 3040
    },
    {
      "epoch": 6.644880174291939,
      "grad_norm": 7.493476390838623,
      "learning_rate": 6.714596949891067e-05,
      "loss": 0.5515,
      "step": 3050
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 8.17051887512207,
      "learning_rate": 6.671023965141613e-05,
      "loss": 0.5864,
      "step": 3060
    },
    {
      "epoch": 6.688453159041394,
      "grad_norm": 4.539484977722168,
      "learning_rate": 6.627450980392157e-05,
      "loss": 0.5484,
      "step": 3070
    },
    {
      "epoch": 6.710239651416122,
      "grad_norm": 7.367282390594482,
      "learning_rate": 6.583877995642702e-05,
      "loss": 0.4757,
      "step": 3080
    },
    {
      "epoch": 6.73202614379085,
      "grad_norm": 4.7097272872924805,
      "learning_rate": 6.540305010893246e-05,
      "loss": 0.5872,
      "step": 3090
    },
    {
      "epoch": 6.753812636165577,
      "grad_norm": 6.816376686096191,
      "learning_rate": 6.496732026143791e-05,
      "loss": 0.4199,
      "step": 3100
    },
    {
      "epoch": 6.775599128540305,
      "grad_norm": 7.247376918792725,
      "learning_rate": 6.453159041394336e-05,
      "loss": 0.3765,
      "step": 3110
    },
    {
      "epoch": 6.7973856209150325,
      "grad_norm": 5.496926784515381,
      "learning_rate": 6.40958605664488e-05,
      "loss": 0.4339,
      "step": 3120
    },
    {
      "epoch": 6.819172113289761,
      "grad_norm": 5.240078449249268,
      "learning_rate": 6.366013071895425e-05,
      "loss": 0.3744,
      "step": 3130
    },
    {
      "epoch": 6.840958605664488,
      "grad_norm": 5.903036117553711,
      "learning_rate": 6.32244008714597e-05,
      "loss": 0.6346,
      "step": 3140
    },
    {
      "epoch": 6.862745098039216,
      "grad_norm": 3.7935237884521484,
      "learning_rate": 6.278867102396514e-05,
      "loss": 0.4666,
      "step": 3150
    },
    {
      "epoch": 6.8845315904139435,
      "grad_norm": 8.861835479736328,
      "learning_rate": 6.23529411764706e-05,
      "loss": 0.5285,
      "step": 3160
    },
    {
      "epoch": 6.906318082788671,
      "grad_norm": 7.999244213104248,
      "learning_rate": 6.191721132897603e-05,
      "loss": 0.5428,
      "step": 3170
    },
    {
      "epoch": 6.928104575163399,
      "grad_norm": 3.581932306289673,
      "learning_rate": 6.148148148148148e-05,
      "loss": 0.5316,
      "step": 3180
    },
    {
      "epoch": 6.949891067538126,
      "grad_norm": 4.369143962860107,
      "learning_rate": 6.104575163398694e-05,
      "loss": 0.7269,
      "step": 3190
    },
    {
      "epoch": 6.9716775599128535,
      "grad_norm": 6.301182270050049,
      "learning_rate": 6.061002178649238e-05,
      "loss": 0.4086,
      "step": 3200
    },
    {
      "epoch": 6.993464052287582,
      "grad_norm": 4.95029354095459,
      "learning_rate": 6.017429193899783e-05,
      "loss": 0.4594,
      "step": 3210
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.871319520174482,
      "eval_loss": 0.44286131858825684,
      "eval_runtime": 17.9948,
      "eval_samples_per_second": 101.918,
      "eval_steps_per_second": 12.781,
      "step": 3213
    },
    {
      "epoch": 7.015250544662309,
      "grad_norm": 6.004560947418213,
      "learning_rate": 5.973856209150327e-05,
      "loss": 0.481,
      "step": 3220
    },
    {
      "epoch": 7.037037037037037,
      "grad_norm": 5.577084064483643,
      "learning_rate": 5.930283224400872e-05,
      "loss": 0.5785,
      "step": 3230
    },
    {
      "epoch": 7.0588235294117645,
      "grad_norm": 4.298581600189209,
      "learning_rate": 5.8867102396514164e-05,
      "loss": 0.5418,
      "step": 3240
    },
    {
      "epoch": 7.080610021786493,
      "grad_norm": 8.312010765075684,
      "learning_rate": 5.843137254901961e-05,
      "loss": 0.4094,
      "step": 3250
    },
    {
      "epoch": 7.10239651416122,
      "grad_norm": 7.940662860870361,
      "learning_rate": 5.799564270152505e-05,
      "loss": 0.3708,
      "step": 3260
    },
    {
      "epoch": 7.124183006535947,
      "grad_norm": 6.090909481048584,
      "learning_rate": 5.755991285403051e-05,
      "loss": 0.4379,
      "step": 3270
    },
    {
      "epoch": 7.1459694989106755,
      "grad_norm": 7.918614387512207,
      "learning_rate": 5.712418300653595e-05,
      "loss": 0.5685,
      "step": 3280
    },
    {
      "epoch": 7.167755991285403,
      "grad_norm": 8.070871353149414,
      "learning_rate": 5.66884531590414e-05,
      "loss": 0.556,
      "step": 3290
    },
    {
      "epoch": 7.189542483660131,
      "grad_norm": 7.955649375915527,
      "learning_rate": 5.625272331154684e-05,
      "loss": 0.3317,
      "step": 3300
    },
    {
      "epoch": 7.211328976034858,
      "grad_norm": 2.451216459274292,
      "learning_rate": 5.581699346405229e-05,
      "loss": 0.4632,
      "step": 3310
    },
    {
      "epoch": 7.233115468409586,
      "grad_norm": 8.610456466674805,
      "learning_rate": 5.538126361655773e-05,
      "loss": 0.3943,
      "step": 3320
    },
    {
      "epoch": 7.254901960784314,
      "grad_norm": 4.276884078979492,
      "learning_rate": 5.4945533769063186e-05,
      "loss": 0.364,
      "step": 3330
    },
    {
      "epoch": 7.276688453159041,
      "grad_norm": 3.7538185119628906,
      "learning_rate": 5.450980392156863e-05,
      "loss": 0.438,
      "step": 3340
    },
    {
      "epoch": 7.298474945533769,
      "grad_norm": 9.271215438842773,
      "learning_rate": 5.4074074074074075e-05,
      "loss": 0.3445,
      "step": 3350
    },
    {
      "epoch": 7.3202614379084965,
      "grad_norm": 2.548752546310425,
      "learning_rate": 5.3638344226579516e-05,
      "loss": 0.3974,
      "step": 3360
    },
    {
      "epoch": 7.342047930283225,
      "grad_norm": 6.424731254577637,
      "learning_rate": 5.320261437908497e-05,
      "loss": 0.4581,
      "step": 3370
    },
    {
      "epoch": 7.363834422657952,
      "grad_norm": 12.000297546386719,
      "learning_rate": 5.2766884531590425e-05,
      "loss": 0.5246,
      "step": 3380
    },
    {
      "epoch": 7.38562091503268,
      "grad_norm": 6.774073123931885,
      "learning_rate": 5.2331154684095866e-05,
      "loss": 0.3998,
      "step": 3390
    },
    {
      "epoch": 7.407407407407407,
      "grad_norm": 6.311558246612549,
      "learning_rate": 5.189542483660131e-05,
      "loss": 0.5616,
      "step": 3400
    },
    {
      "epoch": 7.429193899782135,
      "grad_norm": 6.027487754821777,
      "learning_rate": 5.1459694989106754e-05,
      "loss": 0.3827,
      "step": 3410
    },
    {
      "epoch": 7.450980392156863,
      "grad_norm": 5.9166340827941895,
      "learning_rate": 5.102396514161221e-05,
      "loss": 0.3563,
      "step": 3420
    },
    {
      "epoch": 7.47276688453159,
      "grad_norm": 4.748681545257568,
      "learning_rate": 5.058823529411765e-05,
      "loss": 0.3116,
      "step": 3430
    },
    {
      "epoch": 7.494553376906318,
      "grad_norm": 8.922689437866211,
      "learning_rate": 5.01525054466231e-05,
      "loss": 0.4713,
      "step": 3440
    },
    {
      "epoch": 7.516339869281046,
      "grad_norm": 10.15025520324707,
      "learning_rate": 4.971677559912854e-05,
      "loss": 0.3686,
      "step": 3450
    },
    {
      "epoch": 7.538126361655774,
      "grad_norm": 3.792567253112793,
      "learning_rate": 4.928104575163399e-05,
      "loss": 0.3893,
      "step": 3460
    },
    {
      "epoch": 7.559912854030501,
      "grad_norm": 4.82889461517334,
      "learning_rate": 4.884531590413944e-05,
      "loss": 0.4877,
      "step": 3470
    },
    {
      "epoch": 7.5816993464052285,
      "grad_norm": 5.136831760406494,
      "learning_rate": 4.840958605664489e-05,
      "loss": 0.3821,
      "step": 3480
    },
    {
      "epoch": 7.603485838779957,
      "grad_norm": 2.345472812652588,
      "learning_rate": 4.797385620915033e-05,
      "loss": 0.4723,
      "step": 3490
    },
    {
      "epoch": 7.625272331154684,
      "grad_norm": 5.0139946937561035,
      "learning_rate": 4.7538126361655776e-05,
      "loss": 0.5074,
      "step": 3500
    },
    {
      "epoch": 7.647058823529412,
      "grad_norm": 8.766992568969727,
      "learning_rate": 4.7102396514161224e-05,
      "loss": 0.5244,
      "step": 3510
    },
    {
      "epoch": 7.668845315904139,
      "grad_norm": 4.743102073669434,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.4286,
      "step": 3520
    },
    {
      "epoch": 7.690631808278867,
      "grad_norm": 7.246201515197754,
      "learning_rate": 4.623093681917212e-05,
      "loss": 0.5021,
      "step": 3530
    },
    {
      "epoch": 7.712418300653595,
      "grad_norm": 2.8523175716400146,
      "learning_rate": 4.579520697167756e-05,
      "loss": 0.5146,
      "step": 3540
    },
    {
      "epoch": 7.734204793028322,
      "grad_norm": 3.4125819206237793,
      "learning_rate": 4.535947712418301e-05,
      "loss": 0.4608,
      "step": 3550
    },
    {
      "epoch": 7.75599128540305,
      "grad_norm": 5.793910503387451,
      "learning_rate": 4.4923747276688455e-05,
      "loss": 0.3817,
      "step": 3560
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 9.300203323364258,
      "learning_rate": 4.44880174291939e-05,
      "loss": 0.3712,
      "step": 3570
    },
    {
      "epoch": 7.799564270152505,
      "grad_norm": 6.232841968536377,
      "learning_rate": 4.405228758169935e-05,
      "loss": 0.4187,
      "step": 3580
    },
    {
      "epoch": 7.821350762527233,
      "grad_norm": 5.687042713165283,
      "learning_rate": 4.361655773420479e-05,
      "loss": 0.385,
      "step": 3590
    },
    {
      "epoch": 7.8431372549019605,
      "grad_norm": 6.308765411376953,
      "learning_rate": 4.318082788671024e-05,
      "loss": 0.4733,
      "step": 3600
    },
    {
      "epoch": 7.864923747276689,
      "grad_norm": 7.677695274353027,
      "learning_rate": 4.274509803921569e-05,
      "loss": 0.4538,
      "step": 3610
    },
    {
      "epoch": 7.886710239651416,
      "grad_norm": 5.865328788757324,
      "learning_rate": 4.2309368191721135e-05,
      "loss": 0.481,
      "step": 3620
    },
    {
      "epoch": 7.908496732026144,
      "grad_norm": 3.6379644870758057,
      "learning_rate": 4.1873638344226576e-05,
      "loss": 0.4762,
      "step": 3630
    },
    {
      "epoch": 7.930283224400871,
      "grad_norm": 8.650821685791016,
      "learning_rate": 4.143790849673203e-05,
      "loss": 0.5184,
      "step": 3640
    },
    {
      "epoch": 7.952069716775599,
      "grad_norm": 5.390223026275635,
      "learning_rate": 4.100217864923748e-05,
      "loss": 0.3235,
      "step": 3650
    },
    {
      "epoch": 7.973856209150327,
      "grad_norm": 6.062657833099365,
      "learning_rate": 4.0566448801742925e-05,
      "loss": 0.5322,
      "step": 3660
    },
    {
      "epoch": 7.995642701525054,
      "grad_norm": 4.863360404968262,
      "learning_rate": 4.013071895424837e-05,
      "loss": 0.3783,
      "step": 3670
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8636859323882224,
      "eval_loss": 0.45599865913391113,
      "eval_runtime": 17.9416,
      "eval_samples_per_second": 102.221,
      "eval_steps_per_second": 12.819,
      "step": 3672
    },
    {
      "epoch": 8.017429193899781,
      "grad_norm": 9.74415397644043,
      "learning_rate": 3.9694989106753814e-05,
      "loss": 0.4765,
      "step": 3680
    },
    {
      "epoch": 8.03921568627451,
      "grad_norm": 3.6128804683685303,
      "learning_rate": 3.925925925925926e-05,
      "loss": 0.4289,
      "step": 3690
    },
    {
      "epoch": 8.061002178649238,
      "grad_norm": 4.717423915863037,
      "learning_rate": 3.882352941176471e-05,
      "loss": 0.3206,
      "step": 3700
    },
    {
      "epoch": 8.082788671023966,
      "grad_norm": 3.9667866230010986,
      "learning_rate": 3.838779956427016e-05,
      "loss": 0.5257,
      "step": 3710
    },
    {
      "epoch": 8.104575163398692,
      "grad_norm": 3.601881980895996,
      "learning_rate": 3.79520697167756e-05,
      "loss": 0.3558,
      "step": 3720
    },
    {
      "epoch": 8.12636165577342,
      "grad_norm": 6.372825622558594,
      "learning_rate": 3.7516339869281045e-05,
      "loss": 0.4162,
      "step": 3730
    },
    {
      "epoch": 8.148148148148149,
      "grad_norm": 3.7808709144592285,
      "learning_rate": 3.708061002178649e-05,
      "loss": 0.4766,
      "step": 3740
    },
    {
      "epoch": 8.169934640522875,
      "grad_norm": 7.471041202545166,
      "learning_rate": 3.664488017429194e-05,
      "loss": 0.3726,
      "step": 3750
    },
    {
      "epoch": 8.191721132897603,
      "grad_norm": 5.983527660369873,
      "learning_rate": 3.620915032679739e-05,
      "loss": 0.3768,
      "step": 3760
    },
    {
      "epoch": 8.213507625272332,
      "grad_norm": 6.994285583496094,
      "learning_rate": 3.577342047930283e-05,
      "loss": 0.495,
      "step": 3770
    },
    {
      "epoch": 8.235294117647058,
      "grad_norm": 2.5262835025787354,
      "learning_rate": 3.533769063180828e-05,
      "loss": 0.4897,
      "step": 3780
    },
    {
      "epoch": 8.257080610021786,
      "grad_norm": 5.227507591247559,
      "learning_rate": 3.4901960784313725e-05,
      "loss": 0.3262,
      "step": 3790
    },
    {
      "epoch": 8.278867102396514,
      "grad_norm": 7.645601749420166,
      "learning_rate": 3.446623093681917e-05,
      "loss": 0.5104,
      "step": 3800
    },
    {
      "epoch": 8.300653594771243,
      "grad_norm": 5.904737949371338,
      "learning_rate": 3.403050108932462e-05,
      "loss": 0.3922,
      "step": 3810
    },
    {
      "epoch": 8.322440087145969,
      "grad_norm": 8.857138633728027,
      "learning_rate": 3.359477124183007e-05,
      "loss": 0.3546,
      "step": 3820
    },
    {
      "epoch": 8.344226579520697,
      "grad_norm": 1.4798226356506348,
      "learning_rate": 3.3159041394335515e-05,
      "loss": 0.3725,
      "step": 3830
    },
    {
      "epoch": 8.366013071895425,
      "grad_norm": 7.470787048339844,
      "learning_rate": 3.272331154684096e-05,
      "loss": 0.3591,
      "step": 3840
    },
    {
      "epoch": 8.387799564270152,
      "grad_norm": 10.459611892700195,
      "learning_rate": 3.228758169934641e-05,
      "loss": 0.3921,
      "step": 3850
    },
    {
      "epoch": 8.40958605664488,
      "grad_norm": 8.349335670471191,
      "learning_rate": 3.185185185185185e-05,
      "loss": 0.3941,
      "step": 3860
    },
    {
      "epoch": 8.431372549019608,
      "grad_norm": 6.580867767333984,
      "learning_rate": 3.14161220043573e-05,
      "loss": 0.4646,
      "step": 3870
    },
    {
      "epoch": 8.453159041394336,
      "grad_norm": 7.35141658782959,
      "learning_rate": 3.098039215686275e-05,
      "loss": 0.4288,
      "step": 3880
    },
    {
      "epoch": 8.474945533769063,
      "grad_norm": 10.449567794799805,
      "learning_rate": 3.0544662309368195e-05,
      "loss": 0.4613,
      "step": 3890
    },
    {
      "epoch": 8.49673202614379,
      "grad_norm": 6.271445274353027,
      "learning_rate": 3.010893246187364e-05,
      "loss": 0.2342,
      "step": 3900
    },
    {
      "epoch": 8.518518518518519,
      "grad_norm": 3.820357084274292,
      "learning_rate": 2.9673202614379087e-05,
      "loss": 0.4369,
      "step": 3910
    },
    {
      "epoch": 8.540305010893245,
      "grad_norm": 6.084255218505859,
      "learning_rate": 2.9237472766884534e-05,
      "loss": 0.4126,
      "step": 3920
    },
    {
      "epoch": 8.562091503267974,
      "grad_norm": 10.96562385559082,
      "learning_rate": 2.880174291938998e-05,
      "loss": 0.4772,
      "step": 3930
    },
    {
      "epoch": 8.583877995642702,
      "grad_norm": 11.128349304199219,
      "learning_rate": 2.8366013071895426e-05,
      "loss": 0.4714,
      "step": 3940
    },
    {
      "epoch": 8.60566448801743,
      "grad_norm": 5.957619667053223,
      "learning_rate": 2.793028322440087e-05,
      "loss": 0.4421,
      "step": 3950
    },
    {
      "epoch": 8.627450980392156,
      "grad_norm": 8.840569496154785,
      "learning_rate": 2.7494553376906318e-05,
      "loss": 0.4839,
      "step": 3960
    },
    {
      "epoch": 8.649237472766885,
      "grad_norm": 7.3805832862854,
      "learning_rate": 2.7058823529411766e-05,
      "loss": 0.4628,
      "step": 3970
    },
    {
      "epoch": 8.671023965141613,
      "grad_norm": 4.027458190917969,
      "learning_rate": 2.662309368191721e-05,
      "loss": 0.4592,
      "step": 3980
    },
    {
      "epoch": 8.69281045751634,
      "grad_norm": 6.863933086395264,
      "learning_rate": 2.6187363834422658e-05,
      "loss": 0.5104,
      "step": 3990
    },
    {
      "epoch": 8.714596949891067,
      "grad_norm": 7.0211710929870605,
      "learning_rate": 2.5751633986928102e-05,
      "loss": 0.3347,
      "step": 4000
    },
    {
      "epoch": 8.736383442265796,
      "grad_norm": 2.9679386615753174,
      "learning_rate": 2.5315904139433556e-05,
      "loss": 0.3236,
      "step": 4010
    },
    {
      "epoch": 8.758169934640524,
      "grad_norm": 5.7957353591918945,
      "learning_rate": 2.4880174291938997e-05,
      "loss": 0.4293,
      "step": 4020
    },
    {
      "epoch": 8.77995642701525,
      "grad_norm": 5.933128833770752,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.3671,
      "step": 4030
    },
    {
      "epoch": 8.801742919389978,
      "grad_norm": 3.6138134002685547,
      "learning_rate": 2.4008714596949893e-05,
      "loss": 0.4622,
      "step": 4040
    },
    {
      "epoch": 8.823529411764707,
      "grad_norm": 10.954245567321777,
      "learning_rate": 2.357298474945534e-05,
      "loss": 0.4683,
      "step": 4050
    },
    {
      "epoch": 8.845315904139433,
      "grad_norm": 7.555846214294434,
      "learning_rate": 2.3137254901960788e-05,
      "loss": 0.3031,
      "step": 4060
    },
    {
      "epoch": 8.867102396514161,
      "grad_norm": 5.6423492431640625,
      "learning_rate": 2.2701525054466232e-05,
      "loss": 0.3979,
      "step": 4070
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 5.906759738922119,
      "learning_rate": 2.226579520697168e-05,
      "loss": 0.3555,
      "step": 4080
    },
    {
      "epoch": 8.910675381263616,
      "grad_norm": 7.250192165374756,
      "learning_rate": 2.1830065359477124e-05,
      "loss": 0.4874,
      "step": 4090
    },
    {
      "epoch": 8.932461873638344,
      "grad_norm": 4.774669647216797,
      "learning_rate": 2.1394335511982572e-05,
      "loss": 0.3325,
      "step": 4100
    },
    {
      "epoch": 8.954248366013072,
      "grad_norm": 5.146200180053711,
      "learning_rate": 2.095860566448802e-05,
      "loss": 0.3642,
      "step": 4110
    },
    {
      "epoch": 8.9760348583878,
      "grad_norm": 4.523768901824951,
      "learning_rate": 2.0522875816993464e-05,
      "loss": 0.3164,
      "step": 4120
    },
    {
      "epoch": 8.997821350762527,
      "grad_norm": 9.014480590820312,
      "learning_rate": 2.008714596949891e-05,
      "loss": 0.4272,
      "step": 4130
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8887677208287895,
      "eval_loss": 0.4003233015537262,
      "eval_runtime": 18.0255,
      "eval_samples_per_second": 101.745,
      "eval_steps_per_second": 12.76,
      "step": 4131
    }
  ],
  "logging_steps": 10,
  "max_steps": 4590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.672295208975155e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
